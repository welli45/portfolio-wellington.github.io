{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<<<<<<< HEAD"
      ],
      "id": "b57aba76"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Otimização usando Gradient Descent em duas variáveis usando Python\"\n",
        "image: ../../Vida-Academica/images/calculo/07-gradiente-descida.png\n",
        "description: \"Anotações do módulo 2 do curso de Cálculo para aprendizado de máquina e ciência de dados\"\n",
        "author: \"Wellington Santos Souza\"\n",
        "date: \"2024-08-04\"\n",
        "format: \n",
        "  html:\n",
        "    code-fold: true\n",
        "    code-copy: true\n",
        "    code-tools: true\n",
        "categories: [\"curso\", \"coursera\", \"Algebra Linear\", \"calculo\", \"Machine Learning\", \"Data Science\", \"Deep Learning\", \"Python\", \"R\", \"Algebra Linear\"]\n",
        "---\n",
        "\n",
        "Método de **Gradient Descent** otimizando algumas funções em duas variáveis.\n",
        "\n",
        "Vamos carregar os pacotes que utilizaremos aqui:"
      ],
      "id": "1b5790ba"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from w2_tools import (plot_f_cont_and_surf, gradient_descent_two_variables, \n",
        "                      f_example_3, dfdx_example_3, dfdy_example_3, \n",
        "                      f_example_4, dfdx_example_4, dfdy_example_4)"
      ],
      "id": "6cc94472",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora vamos explorar um exemplo simples de uma função em duas variáveis $f(x,y)$ com um mínimo global."
      ],
      "id": "d0d2346c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_f_cont_and_surf([0, 5], [0, 5], [74, 85], f_example_3, cmap='coolwarm', view={'azim':-60,'elev':28})"
      ],
      "id": "c294f511",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para encontrarmos o mínimo, você podemos implementar a **gradient descent** a partir do ponto inicial $(x_0,y_0)$ e realizar etapas iteração por iteração usando as seguintes equações:\n",
        "\n",
        "$$\n",
        "x_1 = x_0 - \\alpha \\frac{\\partial f}{\\partial x}(x_0, y_0),\n",
        "$$\n",
        "\n",
        "$$\n",
        "y_1 = y_0 - \\alpha \\frac{\\partial f}{\\partial y}(x_0, y_0),\\tag{1}\n",
        "$$\n",
        "\n",
        "em que $\\alpha>0$ é uma taxa de aprendizado. O número de iterações também é um parâmetro. O método é implementado com o seguinte código:"
      ],
      "id": "caf94014"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def gradient_descent(dfdx, dfdy, x, y, learning_rate = 0.1, num_iterations = 100):\n",
        "    for iteration in range(num_iterations):\n",
        "        x, y = x - learning_rate * dfdx(x, y), y - learning_rate * dfdy(x, y)\n",
        "    return x, y"
      ],
      "id": "96846315",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora, para otimizar a função, configuramos os parâmetros `num_iterations`, `learning_rate`, `x_initial`, `y_initial` e executar o **gradient descent**:"
      ],
      "id": "81594dea"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "num_iterations = 30; learning_rate = 0.25; x_initial = 0.5; y_initial = 0.6\n",
        "print(\"Gradient descent result: x_min, y_min =\", \n",
        "      gradient_descent(dfdx_example_3, dfdy_example_3, x_initial, y_initial, learning_rate, num_iterations)) "
      ],
      "id": "6a9877b3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pode ver a visualização executando o código a seguir. Observe que a descida de gradiente em duas variáveis executa etapas no plano, em uma direção oposta ao vetor de gradiente $\\begin{bmatrix}\\frac{\\partial f}{\\partial x}(x_0, y_0) \\\\ \\frac{\\partial f}{\\partial y}(x_0, y_0)\\end{bmatrix}$ com a taxa de aprendizado $\\alpha$ como um fator de escala."
      ],
      "id": "145e0f13"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "num_iterations = 20; learning_rate = 0.25; x_initial = 0.5; y_initial = 0.6\n",
        "num_iterations = 20; learning_rate = 0.5; x_initial = 0.5; y_initial = 0.6\n",
        "num_iterations = 20; learning_rate = 0.15; x_initial = 0.5; y_initial = 0.6\n",
        "num_iterations = 20; learning_rate = 0.15; x_initial = 3.5; y_initial = 3.6\n",
        "\n",
        "gd_example_3 = gradient_descent_two_variables([0, 5], [0, 5], [74, 85], \n",
        "                                              f_example_3, dfdx_example_3, dfdy_example_3, \n",
        "                                              gradient_descent, num_iterations, learning_rate, \n",
        "                                              x_initial, y_initial, \n",
        "                                              [0.1, 0.1, 81.5], 2, [4, 1, 171], \n",
        "                                              cmap='coolwarm', view={'azim':-60,'elev':28})"
      ],
      "id": "3111d96b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos dar uma olhada nessa outra função"
      ],
      "id": "5989e9f3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_f_cont_and_surf([0, 5], [0, 5], [6, 9.5], f_example_4, cmap='terrain', view={'azim':-63,'elev':21})"
      ],
      "id": "e48b5626",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos encontrar o mínimo global com:"
      ],
      "id": "73615357"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "num_iterations = 100; learning_rate = 0.2; x_initial = 0.5; y_initial = 3\n",
        "\n",
        "print(\"Gradient descent result: x_min, y_min =\", \n",
        "      gradient_descent(dfdx_example_4, dfdy_example_4, x_initial, y_initial, learning_rate, num_iterations)) "
      ],
      "id": "0e41efd1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualizando"
      ],
      "id": "99218b25"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Converges to the global minimum point.\n",
        "num_iterations = 30; learning_rate = 0.2; x_initial = 0.5; y_initial = 3\n",
        "# Converges to a local minimum point.\n",
        "# num_iterations = 20; learning_rate = 0.2; x_initial = 2; y_initial = 3\n",
        "# Converges to another local minimum point.\n",
        "# num_iterations = 20; learning_rate = 0.2; x_initial = 4; y_initial = 0.5\n",
        "\n",
        "gd_example_4 = gradient_descent_two_variables([0, 5], [0, 5], [6, 9.5], \n",
        "                                              f_example_4, dfdx_example_4, dfdy_example_4, \n",
        "                                              gradient_descent, num_iterations, learning_rate, \n",
        "                                              x_initial, y_initial, \n",
        "                                              [2, 2, 6], 0.5, [2, 1, 63], \n",
        "                                              cmap='terrain', view={'azim':-63,'elev':21})"
      ],
      "id": "fa375bc9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "=======\n",
        "---"
      ],
      "id": "9a95dd00"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "title: \"Otimização usando Gradient Descent em duas variáveis usando Python\"\n",
        "image: ../../Vida-Academica/images/calculo/07-gradiente-descida.png\n",
        "description: \"Anotações do módulo 2 do curso de Cálculo para aprendizado de máquina e ciência de dados\"\n",
        "author: \"Wellington Santos Souza\"\n",
        "date: \"2024-08-04\"\n",
        "format: \n",
        "  html:\n",
        "    code-fold: true\n",
        "    code-copy: true\n",
        "    code-tools: true\n",
        "categories: [\"curso\", \"coursera\", \"Algebra Linear\", \"calculo\", \"Machine Learning\", \"Data Science\", \"Deep Learning\", \"Python\", \"R\", \"Algebra Linear\"]\n",
        "---\n",
        "\n",
        "Método de **Gradient Descent** otimizando algumas funções em duas variáveis.\n",
        "\n",
        "Vamos carregar os pacotes que utilizaremos aqui:"
      ],
      "id": "c0f06b75"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from w2_tools import (plot_f_cont_and_surf, gradient_descent_two_variables, \n",
        "                      f_example_3, dfdx_example_3, dfdy_example_3, \n",
        "                      f_example_4, dfdx_example_4, dfdy_example_4)"
      ],
      "id": "c6eb9afc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora vamos explorar um exemplo simples de uma função em duas variáveis $f(x,y)$ com um mínimo global."
      ],
      "id": "72057fe3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_f_cont_and_surf([0, 5], [0, 5], [74, 85], f_example_3, cmap='coolwarm', view={'azim':-60,'elev':28})"
      ],
      "id": "9f203ccc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para encontrarmos o mínimo, você podemos implementar a **gradient descent** a partir do ponto inicial $(x_0,y_0)$ e realizar etapas iteração por iteração usando as seguintes equações:\n",
        "\n",
        "$$\n",
        "x_1 = x_0 - \\alpha \\frac{\\partial f}{\\partial x}(x_0, y_0),\n",
        "$$\n",
        "\n",
        "$$\n",
        "y_1 = y_0 - \\alpha \\frac{\\partial f}{\\partial y}(x_0, y_0),\\tag{1}\n",
        "$$\n",
        "\n",
        "em que $\\alpha>0$ é uma taxa de aprendizado. O número de iterações também é um parâmetro. O método é implementado com o seguinte código:"
      ],
      "id": "85e5b060"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def gradient_descent(dfdx, dfdy, x, y, learning_rate = 0.1, num_iterations = 100):\n",
        "    for iteration in range(num_iterations):\n",
        "        x, y = x - learning_rate * dfdx(x, y), y - learning_rate * dfdy(x, y)\n",
        "    return x, y"
      ],
      "id": "b980cce6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora, para otimizar a função, configuramos os parâmetros `num_iterations`, `learning_rate`, `x_initial`, `y_initial` e executar o **gradient descent**:"
      ],
      "id": "380d374d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "num_iterations = 30; learning_rate = 0.25; x_initial = 0.5; y_initial = 0.6\n",
        "print(\"Gradient descent result: x_min, y_min =\", \n",
        "      gradient_descent(dfdx_example_3, dfdy_example_3, x_initial, y_initial, learning_rate, num_iterations)) "
      ],
      "id": "2a69a574",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pode ver a visualização executando o código a seguir. Observe que a descida de gradiente em duas variáveis executa etapas no plano, em uma direção oposta ao vetor de gradiente $\\begin{bmatrix}\\frac{\\partial f}{\\partial x}(x_0, y_0) \\\\ \\frac{\\partial f}{\\partial y}(x_0, y_0)\\end{bmatrix}$ com a taxa de aprendizado $\\alpha$ como um fator de escala."
      ],
      "id": "4ce5dfe9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "num_iterations = 20; learning_rate = 0.25; x_initial = 0.5; y_initial = 0.6\n",
        "num_iterations = 20; learning_rate = 0.5; x_initial = 0.5; y_initial = 0.6\n",
        "num_iterations = 20; learning_rate = 0.15; x_initial = 0.5; y_initial = 0.6\n",
        "num_iterations = 20; learning_rate = 0.15; x_initial = 3.5; y_initial = 3.6\n",
        "\n",
        "gd_example_3 = gradient_descent_two_variables([0, 5], [0, 5], [74, 85], \n",
        "                                              f_example_3, dfdx_example_3, dfdy_example_3, \n",
        "                                              gradient_descent, num_iterations, learning_rate, \n",
        "                                              x_initial, y_initial, \n",
        "                                              [0.1, 0.1, 81.5], 2, [4, 1, 171], \n",
        "                                              cmap='coolwarm', view={'azim':-60,'elev':28})"
      ],
      "id": "a367a18f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos dar uma olhada nessa outra função"
      ],
      "id": "5288d35d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_f_cont_and_surf([0, 5], [0, 5], [6, 9.5], f_example_4, cmap='terrain', view={'azim':-63,'elev':21})"
      ],
      "id": "e80a4871",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos encontrar o mínimo global com:"
      ],
      "id": "bccbb4a8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "num_iterations = 100; learning_rate = 0.2; x_initial = 0.5; y_initial = 3\n",
        "\n",
        "print(\"Gradient descent result: x_min, y_min =\", \n",
        "      gradient_descent(dfdx_example_4, dfdy_example_4, x_initial, y_initial, learning_rate, num_iterations)) "
      ],
      "id": "b6571d6e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualizando"
      ],
      "id": "e8f536a1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Converges to the global minimum point.\n",
        "num_iterations = 30; learning_rate = 0.2; x_initial = 0.5; y_initial = 3\n",
        "# Converges to a local minimum point.\n",
        "# num_iterations = 20; learning_rate = 0.2; x_initial = 2; y_initial = 3\n",
        "# Converges to another local minimum point.\n",
        "# num_iterations = 20; learning_rate = 0.2; x_initial = 4; y_initial = 0.5\n",
        "\n",
        "gd_example_4 = gradient_descent_two_variables([0, 5], [0, 5], [6, 9.5], \n",
        "                                              f_example_4, dfdx_example_4, dfdy_example_4, \n",
        "                                              gradient_descent, num_iterations, learning_rate, \n",
        "                                              x_initial, y_initial, \n",
        "                                              [2, 2, 6], 0.5, [2, 1, 63], \n",
        "                                              cmap='terrain', view={'azim':-63,'elev':21})"
      ],
      "id": "9cc7f026",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">>>>>>> 021b1e17f88b8fa0d99a6d41a52c0b3b9d2c6f5d"
      ],
      "id": "a3e943e7"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\welli\\AppData\\Local\\Programs\\Python\\Python313\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}