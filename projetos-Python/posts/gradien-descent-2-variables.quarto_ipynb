{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<<<<<<< HEAD"
      ],
      "id": "8f0f98e3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Otimização usando Gradient Descent em duas variáveis usando Python\"\n",
        "image: ../../Vida-Academica/images/calculo/07-gradiente-descida.png\n",
        "description: \"Anotações do módulo 2 do curso de Cálculo para aprendizado de máquina e ciência de dados\"\n",
        "author: \"Wellington Santos Souza\"\n",
        "date: \"2024-08-04\"\n",
        "format: \n",
        "  html:\n",
        "    code-fold: true\n",
        "    code-copy: true\n",
        "    code-tools: true\n",
        "categories: [\"curso\", \"coursera\", \"Algebra Linear\", \"calculo\", \"Machine Learning\", \"Data Science\", \"Deep Learning\", \"Python\", \"R\", \"Algebra Linear\"]\n",
        "---\n",
        "\n",
        "\n",
        "Método de **Gradient Descent** otimizando algumas funções em duas variáveis.\n",
        "\n",
        "Vamos carregar os pacotes que utilizaremos aqui:\n"
      ],
      "id": "3b570f66"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from w2_tools import (plot_f_cont_and_surf, gradient_descent_two_variables, \n",
        "                      f_example_3, dfdx_example_3, dfdy_example_3, \n",
        "                      f_example_4, dfdx_example_4, dfdy_example_4)"
      ],
      "id": "503598a0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora vamos explorar um exemplo simples de uma função em duas variáveis $f(x,y)$ com um mínimo global.\n"
      ],
      "id": "0f99c1f3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_f_cont_and_surf([0, 5], [0, 5], [74, 85], f_example_3, cmap='coolwarm', view={'azim':-60,'elev':28})"
      ],
      "id": "8f87b5a6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para encontrarmos o mínimo, você podemos implementar a **gradient descent** a partir do ponto inicial $(x_0,y_0)$ e realizar etapas iteração por iteração usando as seguintes equações:\n",
        "\n",
        "$$\n",
        "x_1 = x_0 - \\alpha \\frac{\\partial f}{\\partial x}(x_0, y_0),\n",
        "$$\n",
        "\n",
        "$$\n",
        "y_1 = y_0 - \\alpha \\frac{\\partial f}{\\partial y}(x_0, y_0),\\tag{1}\n",
        "$$\n",
        "\n",
        "em que $\\alpha>0$ é uma taxa de aprendizado. O número de iterações também é um parâmetro. O método é implementado com o seguinte código:\n"
      ],
      "id": "49bb7bba"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def gradient_descent(dfdx, dfdy, x, y, learning_rate = 0.1, num_iterations = 100):\n",
        "    for iteration in range(num_iterations):\n",
        "        x, y = x - learning_rate * dfdx(x, y), y - learning_rate * dfdy(x, y)\n",
        "    return x, y"
      ],
      "id": "1f91b59e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora, para otimizar a função, configuramos os parâmetros `num_iterations`, `learning_rate`, `x_initial`, `y_initial` e executar o **gradient descent**:\n"
      ],
      "id": "ac7c6a4e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "num_iterations = 30; learning_rate = 0.25; x_initial = 0.5; y_initial = 0.6\n",
        "print(\"Gradient descent result: x_min, y_min =\", \n",
        "      gradient_descent(dfdx_example_3, dfdy_example_3, x_initial, y_initial, learning_rate, num_iterations)) "
      ],
      "id": "3eab0f80",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pode ver a visualização executando o código a seguir. Observe que a descida de gradiente em duas variáveis executa etapas no plano, em uma direção oposta ao vetor de gradiente $\\begin{bmatrix}\\frac{\\partial f}{\\partial x}(x_0, y_0) \\\\ \\frac{\\partial f}{\\partial y}(x_0, y_0)\\end{bmatrix}$ com a taxa de aprendizado $\\alpha$ como um fator de escala.\n"
      ],
      "id": "8c1b0605"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "num_iterations = 20; learning_rate = 0.25; x_initial = 0.5; y_initial = 0.6\n",
        "num_iterations = 20; learning_rate = 0.5; x_initial = 0.5; y_initial = 0.6\n",
        "num_iterations = 20; learning_rate = 0.15; x_initial = 0.5; y_initial = 0.6\n",
        "num_iterations = 20; learning_rate = 0.15; x_initial = 3.5; y_initial = 3.6\n",
        "\n",
        "gd_example_3 = gradient_descent_two_variables([0, 5], [0, 5], [74, 85], \n",
        "                                              f_example_3, dfdx_example_3, dfdy_example_3, \n",
        "                                              gradient_descent, num_iterations, learning_rate, \n",
        "                                              x_initial, y_initial, \n",
        "                                              [0.1, 0.1, 81.5], 2, [4, 1, 171], \n",
        "                                              cmap='coolwarm', view={'azim':-60,'elev':28})"
      ],
      "id": "dd3784f7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos dar uma olhada nessa outra função\n"
      ],
      "id": "86d70916"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_f_cont_and_surf([0, 5], [0, 5], [6, 9.5], f_example_4, cmap='terrain', view={'azim':-63,'elev':21})"
      ],
      "id": "38f996a5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos encontrar o mínimo global com:\n"
      ],
      "id": "0e507666"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "num_iterations = 100; learning_rate = 0.2; x_initial = 0.5; y_initial = 3\n",
        "\n",
        "print(\"Gradient descent result: x_min, y_min =\", \n",
        "      gradient_descent(dfdx_example_4, dfdy_example_4, x_initial, y_initial, learning_rate, num_iterations)) "
      ],
      "id": "1669a5c9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualizando\n"
      ],
      "id": "003f8125"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Converges to the global minimum point.\n",
        "num_iterations = 30; learning_rate = 0.2; x_initial = 0.5; y_initial = 3\n",
        "# Converges to a local minimum point.\n",
        "# num_iterations = 20; learning_rate = 0.2; x_initial = 2; y_initial = 3\n",
        "# Converges to another local minimum point.\n",
        "# num_iterations = 20; learning_rate = 0.2; x_initial = 4; y_initial = 0.5\n",
        "\n",
        "gd_example_4 = gradient_descent_two_variables([0, 5], [0, 5], [6, 9.5], \n",
        "                                              f_example_4, dfdx_example_4, dfdy_example_4, \n",
        "                                              gradient_descent, num_iterations, learning_rate, \n",
        "                                              x_initial, y_initial, \n",
        "                                              [2, 2, 6], 0.5, [2, 1, 63], \n",
        "                                              cmap='terrain', view={'azim':-63,'elev':21})"
      ],
      "id": "b3d95b44",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "=======\n",
        "\n",
        "---"
      ],
      "id": "e0e7fa98"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "title: \"Otimização usando Gradient Descent em duas variáveis usando Python\"\n",
        "image: ../../Vida-Academica/images/calculo/07-gradiente-descida.png\n",
        "description: \"Anotações do módulo 2 do curso de Cálculo para aprendizado de máquina e ciência de dados\"\n",
        "author: \"Wellington Santos Souza\"\n",
        "date: \"2024-08-04\"\n",
        "format: \n",
        "  html:\n",
        "    code-fold: true\n",
        "    code-copy: true\n",
        "    code-tools: true\n",
        "categories: [\"curso\", \"coursera\", \"Algebra Linear\", \"calculo\", \"Machine Learning\", \"Data Science\", \"Deep Learning\", \"Python\", \"R\", \"Algebra Linear\"]\n",
        "---\n",
        "\n",
        "\n",
        "Método de **Gradient Descent** otimizando algumas funções em duas variáveis.\n",
        "\n",
        "Vamos carregar os pacotes que utilizaremos aqui:\n"
      ],
      "id": "9bad5d78"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from w2_tools import (plot_f_cont_and_surf, gradient_descent_two_variables, \n",
        "                      f_example_3, dfdx_example_3, dfdy_example_3, \n",
        "                      f_example_4, dfdx_example_4, dfdy_example_4)"
      ],
      "id": "768d9e97",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora vamos explorar um exemplo simples de uma função em duas variáveis $f(x,y)$ com um mínimo global.\n"
      ],
      "id": "e59308b4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_f_cont_and_surf([0, 5], [0, 5], [74, 85], f_example_3, cmap='coolwarm', view={'azim':-60,'elev':28})"
      ],
      "id": "26045c7e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para encontrarmos o mínimo, você podemos implementar a **gradient descent** a partir do ponto inicial $(x_0,y_0)$ e realizar etapas iteração por iteração usando as seguintes equações:\n",
        "\n",
        "$$\n",
        "x_1 = x_0 - \\alpha \\frac{\\partial f}{\\partial x}(x_0, y_0),\n",
        "$$\n",
        "\n",
        "$$\n",
        "y_1 = y_0 - \\alpha \\frac{\\partial f}{\\partial y}(x_0, y_0),\\tag{1}\n",
        "$$\n",
        "\n",
        "em que $\\alpha>0$ é uma taxa de aprendizado. O número de iterações também é um parâmetro. O método é implementado com o seguinte código:\n"
      ],
      "id": "6a06ffcc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def gradient_descent(dfdx, dfdy, x, y, learning_rate = 0.1, num_iterations = 100):\n",
        "    for iteration in range(num_iterations):\n",
        "        x, y = x - learning_rate * dfdx(x, y), y - learning_rate * dfdy(x, y)\n",
        "    return x, y"
      ],
      "id": "6140d4dc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Agora, para otimizar a função, configuramos os parâmetros `num_iterations`, `learning_rate`, `x_initial`, `y_initial` e executar o **gradient descent**:\n"
      ],
      "id": "a49cc349"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "num_iterations = 30; learning_rate = 0.25; x_initial = 0.5; y_initial = 0.6\n",
        "print(\"Gradient descent result: x_min, y_min =\", \n",
        "      gradient_descent(dfdx_example_3, dfdy_example_3, x_initial, y_initial, learning_rate, num_iterations)) "
      ],
      "id": "f1b2421c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pode ver a visualização executando o código a seguir. Observe que a descida de gradiente em duas variáveis executa etapas no plano, em uma direção oposta ao vetor de gradiente $\\begin{bmatrix}\\frac{\\partial f}{\\partial x}(x_0, y_0) \\\\ \\frac{\\partial f}{\\partial y}(x_0, y_0)\\end{bmatrix}$ com a taxa de aprendizado $\\alpha$ como um fator de escala.\n"
      ],
      "id": "b42c11a9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "num_iterations = 20; learning_rate = 0.25; x_initial = 0.5; y_initial = 0.6\n",
        "num_iterations = 20; learning_rate = 0.5; x_initial = 0.5; y_initial = 0.6\n",
        "num_iterations = 20; learning_rate = 0.15; x_initial = 0.5; y_initial = 0.6\n",
        "num_iterations = 20; learning_rate = 0.15; x_initial = 3.5; y_initial = 3.6\n",
        "\n",
        "gd_example_3 = gradient_descent_two_variables([0, 5], [0, 5], [74, 85], \n",
        "                                              f_example_3, dfdx_example_3, dfdy_example_3, \n",
        "                                              gradient_descent, num_iterations, learning_rate, \n",
        "                                              x_initial, y_initial, \n",
        "                                              [0.1, 0.1, 81.5], 2, [4, 1, 171], \n",
        "                                              cmap='coolwarm', view={'azim':-60,'elev':28})"
      ],
      "id": "e5a1d5f7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos dar uma olhada nessa outra função\n"
      ],
      "id": "703a7132"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plot_f_cont_and_surf([0, 5], [0, 5], [6, 9.5], f_example_4, cmap='terrain', view={'azim':-63,'elev':21})"
      ],
      "id": "fc3ac9cf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos encontrar o mínimo global com:\n"
      ],
      "id": "629b234e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "num_iterations = 100; learning_rate = 0.2; x_initial = 0.5; y_initial = 3\n",
        "\n",
        "print(\"Gradient descent result: x_min, y_min =\", \n",
        "      gradient_descent(dfdx_example_4, dfdy_example_4, x_initial, y_initial, learning_rate, num_iterations)) "
      ],
      "id": "5e97207b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualizando\n"
      ],
      "id": "d6ab319e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Converges to the global minimum point.\n",
        "num_iterations = 30; learning_rate = 0.2; x_initial = 0.5; y_initial = 3\n",
        "# Converges to a local minimum point.\n",
        "# num_iterations = 20; learning_rate = 0.2; x_initial = 2; y_initial = 3\n",
        "# Converges to another local minimum point.\n",
        "# num_iterations = 20; learning_rate = 0.2; x_initial = 4; y_initial = 0.5\n",
        "\n",
        "gd_example_4 = gradient_descent_two_variables([0, 5], [0, 5], [6, 9.5], \n",
        "                                              f_example_4, dfdx_example_4, dfdy_example_4, \n",
        "                                              gradient_descent, num_iterations, learning_rate, \n",
        "                                              x_initial, y_initial, \n",
        "                                              [2, 2, 6], 0.5, [2, 1, 63], \n",
        "                                              cmap='terrain', view={'azim':-63,'elev':21})"
      ],
      "id": "2b1b97e6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">>>>>>> 021b1e17f88b8fa0d99a6d41a52c0b3b9d2c6f5d"
      ],
      "id": "d39f364c"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\welli\\AppData\\Local\\Programs\\Python\\Python313\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}